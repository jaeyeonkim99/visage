# Experiment Config for each experiment
output_dir: /output
logging_dir: runs
logging_steps: 10
seed: 1115
train_file: csv/yt_ambigen/train.csv
validation_file: csv/yt_ambigen/valid.csv
demo_file: csv/yt_ambigen/demo.csv
demo_metadata: csv/yt_ambigen/demo_meta.csv
dac_base_path: /data/yt_ambigen/dac
rotation_base_path: /data/yt_ambigen/dac_rotated # null for training wihtout rotation augmentation
clip_base_path: /data/yt_ambigen/clip
energy_map_path: /data/yt_ambigen/energy_map
model_cfg: cfg/config.json
model_path: /data/vgg_checkpoint
overwrite_output_dir: False

# Basic Config
num_train_epochs: 80
max_train_steps: null
seconds_to_use: 5
gradient_accumulation_steps: 1
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
split_batches: true
checkpointing_steps: epoch  # 'epoch' to save for each epoch, or number of steps
resume_from_checkpoint: null

# Training Hyperparameters
# "lr_schedulre_type" should be one of "linear", "cosine", "cosine_with_restarts", "polynomial", 
# "constant", "constant_with_warmpup", "inverse_sqrt", "reduce_lr_on_plateau", "two_stage_inverse_sqrt"
# lr_scheduler_type: constant_with_warmup
lr_scheduler_type: constant
learning_rate: 1e-4  # peak lr
num_warmup_steps: 0
weight_decay: 0.01
max_grad_norm: 1.0

# Others
with_tracking: true
report_to: tensorboard
ignore_pad_token_for_loss: true 
preprocessing_num_workers: 32
overwrite_cache: false